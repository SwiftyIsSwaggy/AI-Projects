{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Fire Net Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my own model using normal convolutions and we will also design one combining this model and depth wise separable convolutions as used in Mobile Net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, for my project we have:\n",
    "-A bench mark model which is a pre trained Mobile net.\n",
    "-A fire net model made up of standard convolution.\n",
    "-A fire net model made up of depthwise convolution.\n",
    "-A model from PyImage Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import shutil\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_URL = 'https://fire-net-datasets.s3.amazonaws.com/Training+Dataset.zip'\n",
    "\n",
    "zip_file = tf.keras.utils.get_file(origin=_URL,extract=True)  \n",
    "#This will ge the file and extract it to a directory and extract to /Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.path.dirname(zip_file))\n",
    "#This function returns the directory of the extracted folder without the extracted folder inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.join(os.path.dirname(zip_file), 'Training Dataset')\n",
    "#A good way to add the directory of the extracted folder and also the extracted folder itself.\n",
    "print(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Fire', 'NoFire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl in classes:\n",
    "  img_path = os.path.join(base_dir, cl)\n",
    "  images = glob.glob(img_path + '/*.jpg')\n",
    "  print(\"{}: {} Images\".format(cl, len(images)))\n",
    "  train, val = images[:round(len(images)*0.7)], images[round(len(images)*0.7):]\n",
    "\n",
    "  for t in train:\n",
    "    if not os.path.exists(os.path.join(base_dir, 'train', cl)):\n",
    "      os.makedirs(os.path.join(base_dir, 'train', cl))\n",
    "    shutil.move(t, os.path.join(base_dir, 'train', cl))\n",
    "\n",
    "  for v in val:\n",
    "    if not os.path.exists(os.path.join(base_dir, 'val', cl)):\n",
    "      os.makedirs(os.path.join(base_dir, 'val', cl))\n",
    "    shutil.move(v, os.path.join(base_dir, 'val', cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "IMG_SHAPE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen_train = ImageDataGenerator(rescale = 1./255,\n",
    "                    zoom_range = 0.5,\n",
    "                    rotation_range=45,\n",
    "                    horizontal_flip=True,\n",
    "                    width_shift_range=0.15,\n",
    "                    height_shift_range=0.15,\n",
    "                    shear_range=0.2)\n",
    "train_data_gen = image_gen_train.flow_from_directory(batch_size=batch_size,\n",
    "                                            directory=train_dir,\n",
    "                                            shuffle=True,\n",
    "                                            target_size=(IMG_SHAPE,IMG_SHAPE),\n",
    "                                            class_mode='binary')\n",
    "print(train_data_gen.samples)\n",
    "print(train_data_gen.n)\n",
    "train_data_num = train_data_gen.samples\n",
    "#Find our size of datasets. each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
    "val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,\n",
    "                            directory=val_dir,\n",
    "                            target_size=(IMG_SHAPE,IMG_SHAPE),\n",
    "                            class_mode='binary',\n",
    "                            shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_data_gen.samples)\n",
    "print(val_data_gen.n)\n",
    "val_data_num = val_data_gen.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, 3, padding = \"same\", activation = 'relu', input_shape = (IMG_SHAPE, IMG_SHAPE, 3)))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(32, 3, padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(64, 3, padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "\n",
    "model.add(Dense(2), activation = 'softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Places to test\n",
    "0. Test with activation after Max Pool. (Performance Speed)\n",
    "1. Overfit first so that you see maximum number of epoch.\n",
    "2. Removing dropout on each layer or changing some of its values. (Overfitting or Not)\n",
    "2. Discard dropout and use batch normalization. (Performance Accuracy)\n",
    "4. Add a Dense layer of 128. (Accuracy)\n",
    "5. MaxPooling to default stride (Accuracy) \n",
    "6. After finding best epoch, use reduce early and stop. (Generalizaation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 80\n",
    "model.compile(optimizer=\"adam\",\n",
    "                loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_data_gen,epochs= EPOCHS,\n",
    "                steps_per_epoch = int(np.ceil(train_data_gen.n / float(batch_size))),\n",
    "                validation_data = val_data_gen,\n",
    "                validation_steps = int(np.ceil(val_data_gen.n / float(batch_size))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix and Classification Report\n",
    "val_data_gen.reset()\n",
    "Y_pred = model.predict_generator(val_data_gen, int(np.ceil(val_data_gen.n / float(batch_size))))\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(val_data_gen.classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = ['Fire', 'No Fire']\n",
    "print(classification_report(val_data_gen.classes, y_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.6 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.6-gpu-py38-cu112-ubuntu20.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
