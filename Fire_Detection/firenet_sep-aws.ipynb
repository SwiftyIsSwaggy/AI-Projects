{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SC9lCieIWihi"
   },
   "source": [
    "# The Fire Net Model trained on AWS SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Depthwise Separable Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnC7eRBOWihr"
   },
   "source": [
    "This is my own model using depthwise separable convolutions. There is a version which has been designed before using normal standard convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Or97TIE-Wihs"
   },
   "source": [
    "Therefore, for my project we have:\n",
    "* A bench mark model which is a pre trained Mobile net.\n",
    "* A fire net model made up of standard convolution.\n",
    "* A fire net model made up of depthwise convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "k7C4n7KqWihu"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a5137ad2f192>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import shutil\n",
    "import tensorboard\n",
    "import tensorflow_hub as hub\n",
    "import datetime\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1KEXI9EWihx"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization, DepthwiseConv2D, AveragePooling2D\n",
    "from sklearn.metrics import classification_report, confusion_matrix,roc_curve,auc, roc_auc_score,precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay,RocCurveDisplay,ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUgCTbwdWihy",
    "outputId": "abb2846e-9fcf-4c0c-ef12-d1c0f9c65652"
   },
   "outputs": [],
   "source": [
    "#initial download to instance. After downloading to S3, no need to run this again.\n",
    "_URL = 'https://fire-net-datasets.s3.amazonaws.com/Training_Dataset.zip'\n",
    "\n",
    "zip_file = tf.keras.utils.get_file(origin=_URL,extract=True)  \n",
    "#This will ge the file and extract it to a directory and extract to /Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfZqTHq0Wih0",
    "outputId": "4ab8347a-868e-42ba-e90d-7745550ba202"
   },
   "outputs": [],
   "source": [
    "print(os.path.dirname(zip_file))\n",
    "#This function returns the directory of the extracted folder without the extracted folder inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBqfaImMWih1",
    "outputId": "83ad734e-b79e-4422-de78-523c051c2ef1"
   },
   "outputs": [],
   "source": [
    "base_dir = os.path.join(os.path.dirname(zip_file), 'data')\n",
    "#A good way to add the directory of the extracted folder and also the extracted folder itself.\n",
    "print(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sT2yDuO5Wih2"
   },
   "outputs": [],
   "source": [
    "classes = ['Fire', 'NoFire']\n",
    "sets = ['train', 'val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yq5L8VBkWih3",
    "outputId": "36993da4-3a31-40fa-8d01-52ac05cec118"
   },
   "outputs": [],
   "source": [
    "for cl in classes:\n",
    "  img_path = os.path.join(base_dir, cl)\n",
    "  images = glob.glob(img_path + '/*.jpg')\n",
    "  print(\"{}: {} Images\".format(cl, len(images)))\n",
    "  train, val = images[:round(len(images)*0.7)], images[round(len(images)*0.7):]\n",
    "\n",
    "  for t in train:\n",
    "    if not os.path.exists(os.path.join(base_dir, 'train', cl)):\n",
    "      os.makedirs(os.path.join(base_dir, 'train', cl))\n",
    "    shutil.move(t, os.path.join(base_dir, 'train', cl))\n",
    "\n",
    "  for v in val:\n",
    "    if not os.path.exists(os.path.join(base_dir, 'val', cl)):\n",
    "      os.makedirs(os.path.join(base_dir, 'val', cl))\n",
    "    shutil.move(v, os.path.join(base_dir, 'val', cl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDeD3hWmWih4"
   },
   "outputs": [],
   "source": [
    "sets_counts = {\n",
    "    'train': 0,\n",
    "    'validation': 0\n",
    "}\n",
    "\n",
    "for set_name in sets:\n",
    "    for class_name in classes:\n",
    "        path = os.path.join(base_dir, set_name, class_name)\n",
    "        count = len(os.listdir(path))\n",
    "        print(path, 'has', count, 'images')\n",
    "        sets_counts[set_name] += count\n",
    "\n",
    "print(sets_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzuW49yMWih4"
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "dataset_bucket = \"swifty-datasets\"\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"firenet_aws\"\n",
    "tensorflow_logs_path = \"s3://{}/{}/logs\".format(bucket, prefix)\n",
    "\n",
    "print(\"Bucket: {}\".format(bucket))\n",
    "print(\"SageMaker ver: \" + sagemaker.__version__)\n",
    "print(\"Tensorflow ver: \" + tf.__version__)\n",
    "\n",
    "print(\"Uploading to S3\")\n",
    "s3_data_path = sess.upload_data(path=base_dir, bucket=dataset_bucket, key_prefix='firenet_data')\n",
    "print(\"Uploaded to\", s3_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_metric_definition = [\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \".*loss: ([0-9\\\\.]+) - accuracy: [0-9\\\\.]+.*\"},\n",
    "    {\"Name\": \"train:accuracy\", \"Regex\": \".*loss: [0-9\\\\.]+ - accuracy: ([0-9\\\\.]+).*\"},\n",
    "    {\n",
    "        \"Name\": \"validation:accuracy\",\n",
    "        \"Regex\": \".*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: ([0-9\\\\.]+).*\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"validation:loss\",\n",
    "        \"Regex\": \".*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_accuracy: [0-9\\\\.]+.*\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"sec/steps\",\n",
    "        \"Regex\": \".* (\\d+)[mu]s/step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: [0-9\\\\.]+\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uMVRSYsWih7"
   },
   "outputs": [],
   "source": [
    "hyper_parameters = {\"epochs\": 10, \"tf-logs-path\": tensorflow_logs_path}\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    entry_point='train.py',\n",
    "    base_job_name=\"firenet-training\",\n",
    "    role=role,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    instance_count=1,\n",
    "    py_version='py38',\n",
    "    framework_version = '2.6.0',\n",
    "    hyperparameters =hyper_parameters,\n",
    "    metric_definitions=keras_metric_definition,\n",
    "    output_path='s3://swifty-ai-models/other_models/firenet_tf_sm/'\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "4IkRmnvpWih8",
    "outputId": "04672084-8339-431c-b32c-53b4ba7381b3"
   },
   "outputs": [],
   "source": [
    "estimator.fit(s3_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0hVUkckWih9",
    "outputId": "0eccddec-c612-4850-edd8-3f3e0d6e9a2d"
   },
   "outputs": [],
   "source": [
    "fire_predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "print('\\nModel is deployed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQbM3f74Wih_",
    "outputId": "7469d2d3-669c-46e4-d44f-4711f749cf24"
   },
   "outputs": [],
   "source": [
    "#Upload images to the test directory first.\n",
    "test_dir = 'Fire_Detection/data/test/'\n",
    "test_images = [os.path.join(test_dir, x) for x in os.listdir(test_dir)]\n",
    "print(test_images[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EAdAe9brWih_"
   },
   "outputs": [],
   "source": [
    "def get_pred(img_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(128, 128))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    results = fire_predictor.predict(img)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CLmkg1bWiiB",
    "outputId": "e4a6715e-ed30-445a-afe1-a5c5e97dcd38"
   },
   "outputs": [],
   "source": [
    "#Getting predictions for all images\n",
    "def showPredictions(image_list):\n",
    "    predicted_classes = []\n",
    "    for i in range(len(image_list)):\n",
    "        image_path = image_list[i]\n",
    "        results = get_pred(image_path)\n",
    "        print(results)\n",
    "        predicted_id = np.argmax(results)\n",
    "        predicted_class = classes[predicted_id]\n",
    "        predicted_classes.append(predicted_class)\n",
    "    plt.figure(figsize=(10,9))\n",
    "    for n in range(len(image_list)):\n",
    "        plt.subplot(6,5,n+1)\n",
    "        plt.subplots_adjust(hspace = 0.3)\n",
    "        image = plt.imread(image_list[n])\n",
    "        plt.imshow(image)\n",
    "        plt.title(predicted_classes[n].title())\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kSg7-6_IWiiC",
    "outputId": "5c696acc-f42b-4388-b36e-11c63123639e"
   },
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(fire_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"learning-rate\": ContinuousParameter(0.00001, 0.001),\n",
    "    \"batch-size\": CategoricalParameter([64, 128]),\n",
    "    \"optimizer\": CategoricalParameter([\"sgd\", \"adam\", \"rmsprop\"]),\n",
    "}\n",
    "\n",
    "objective_metric_name = \"validation:accuracy\"\n",
    "\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions=keras_metric_definition,\n",
    "    objective_type=\"Maximize\",\n",
    "    max_jobs=4,\n",
    "    max_parallel_jobs=2,\n",
    "    early_stopping_type=\"Auto\",\n",
    "    base_tuning_job_name=\"firenet-hpo-tuning\",\n",
    ")\n",
    "\n",
    "tuner.fit(s3_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paste the command that is the output of the next cell to start your tensorboard instance on Studio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_region = sagemaker_session.boto_region_name\n",
    "!AWS_REGION={aws_region}\n",
    "!echo tensorboard --logdir {tensorflow_logs_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instance of TensorBoard will be available at https://<notebook instance hostname>/proxy/6006/. By default TensorBoard assigns port 6006, but if it’s already in use TensorBoard will increase the port by 1, so 6007, 6008 and so on until it finds an available port."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "firenet.ipynb",
   "provenance": []
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.1 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.1-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
